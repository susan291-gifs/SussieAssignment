{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susan291-gifs/SussieAssignment/blob/main/Tensorflow_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ba5L9sxqrSr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R37PZX9qz5E"
      },
      "source": [
        "###Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGV9SVtaR6UL"
      },
      "source": [
        "1. The weights had to be initialized\n",
        "2. Forward propagation matrix calculation was required\n",
        "3. Backpropagation matrix calculations were required\n",
        "4. Epoch loop was required\n",
        "5. Losses needed to be calculated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Lp74_-iXST"
      },
      "source": [
        "###Problem 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owE_CMvMUrgy",
        "outputId": "8770e27c-bd47-4be2-d65f-c0acbb4d2bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss: 0.7239, val_loss: 0.6971, acc: 0.521\n",
            "Epoch 2, loss: 0.6626, val_loss: 0.7271, acc: 0.550\n",
            "Epoch 3, loss: 0.6478, val_loss: 0.7041, acc: 0.550\n",
            "Epoch 4, loss: 0.6328, val_loss: 0.6752, acc: 0.550\n",
            "Epoch 5, loss: 0.6227, val_loss: 0.6588, acc: 0.550\n",
            "Epoch 6, loss: 0.6136, val_loss: 0.6522, acc: 0.579\n",
            "Epoch 7, loss: 0.6035, val_loss: 0.6463, acc: 0.579\n",
            "Epoch 8, loss: 0.5923, val_loss: 0.6349, acc: 0.579\n",
            "Epoch 9, loss: 0.5796, val_loss: 0.6184, acc: 0.664\n",
            "Epoch 10, loss: 0.5648, val_loss: 0.6007, acc: 0.743\n",
            "Epoch 11, loss: 0.5497, val_loss: 0.5856, acc: 0.757\n",
            "Epoch 12, loss: 0.5350, val_loss: 0.5703, acc: 0.786\n",
            "Epoch 13, loss: 0.5206, val_loss: 0.5540, acc: 0.814\n",
            "Epoch 14, loss: 0.5067, val_loss: 0.5388, acc: 0.843\n",
            "Epoch 15, loss: 0.4929, val_loss: 0.5244, acc: 0.871\n",
            "Epoch 16, loss: 0.4791, val_loss: 0.5096, acc: 0.914\n",
            "Epoch 17, loss: 0.4655, val_loss: 0.4946, acc: 0.929\n",
            "Epoch 18, loss: 0.4520, val_loss: 0.4797, acc: 0.929\n",
            "Epoch 19, loss: 0.4385, val_loss: 0.4648, acc: 0.929\n",
            "Epoch 20, loss: 0.4253, val_loss: 0.4500, acc: 0.929\n",
            "Epoch 21, loss: 0.4122, val_loss: 0.4356, acc: 0.929\n",
            "Epoch 22, loss: 0.3993, val_loss: 0.4213, acc: 0.929\n",
            "Epoch 23, loss: 0.3867, val_loss: 0.4074, acc: 0.957\n",
            "Epoch 24, loss: 0.3743, val_loss: 0.3938, acc: 0.957\n",
            "Epoch 25, loss: 0.3621, val_loss: 0.3804, acc: 0.957\n",
            "Epoch 26, loss: 0.3503, val_loss: 0.3674, acc: 0.971\n",
            "Epoch 27, loss: 0.3388, val_loss: 0.3547, acc: 0.971\n",
            "Epoch 28, loss: 0.3277, val_loss: 0.3424, acc: 0.971\n",
            "Epoch 29, loss: 0.3169, val_loss: 0.3303, acc: 0.971\n",
            "Epoch 30, loss: 0.3066, val_loss: 0.3188, acc: 0.971\n",
            "Epoch 31, loss: 0.2966, val_loss: 0.3076, acc: 0.971\n",
            "Epoch 32, loss: 0.2870, val_loss: 0.2968, acc: 0.971\n",
            "Epoch 33, loss: 0.2778, val_loss: 0.2866, acc: 0.971\n",
            "Epoch 34, loss: 0.2689, val_loss: 0.2767, acc: 0.971\n",
            "Epoch 35, loss: 0.2605, val_loss: 0.2670, acc: 0.971\n",
            "Epoch 36, loss: 0.2524, val_loss: 0.2580, acc: 0.971\n",
            "Epoch 37, loss: 0.2447, val_loss: 0.2493, acc: 0.971\n",
            "Epoch 38, loss: 0.2374, val_loss: 0.2411, acc: 0.971\n",
            "Epoch 39, loss: 0.2304, val_loss: 0.2331, acc: 0.971\n",
            "Epoch 40, loss: 0.2237, val_loss: 0.2254, acc: 0.971\n",
            "Epoch 41, loss: 0.2175, val_loss: 0.2184, acc: 0.971\n",
            "Epoch 42, loss: 0.2114, val_loss: 0.2116, acc: 0.971\n",
            "Epoch 43, loss: 0.2057, val_loss: 0.2050, acc: 0.971\n",
            "Epoch 44, loss: 0.2002, val_loss: 0.1988, acc: 0.971\n",
            "Epoch 45, loss: 0.1951, val_loss: 0.1929, acc: 0.971\n",
            "Epoch 46, loss: 0.1901, val_loss: 0.1873, acc: 0.971\n",
            "Epoch 47, loss: 0.1854, val_loss: 0.1819, acc: 0.971\n",
            "Epoch 48, loss: 0.1809, val_loss: 0.1768, acc: 0.971\n",
            "Epoch 49, loss: 0.1767, val_loss: 0.1720, acc: 0.971\n",
            "Epoch 50, loss: 0.1726, val_loss: 0.1674, acc: 0.971\n",
            "Epoch 51, loss: 0.1687, val_loss: 0.1629, acc: 0.971\n",
            "Epoch 52, loss: 0.1650, val_loss: 0.1588, acc: 0.971\n",
            "Epoch 53, loss: 0.1615, val_loss: 0.1548, acc: 0.971\n",
            "Epoch 54, loss: 0.1581, val_loss: 0.1508, acc: 0.971\n",
            "Epoch 55, loss: 0.1550, val_loss: 0.1471, acc: 0.971\n",
            "Epoch 56, loss: 0.1519, val_loss: 0.1435, acc: 0.971\n",
            "Epoch 57, loss: 0.1490, val_loss: 0.1402, acc: 0.971\n",
            "Epoch 58, loss: 0.1461, val_loss: 0.1369, acc: 0.971\n",
            "Epoch 59, loss: 0.1434, val_loss: 0.1338, acc: 0.971\n",
            "Epoch 60, loss: 0.1409, val_loss: 0.1309, acc: 0.971\n",
            "Epoch 61, loss: 0.1384, val_loss: 0.1280, acc: 0.971\n",
            "Epoch 62, loss: 0.1360, val_loss: 0.1252, acc: 0.971\n",
            "Epoch 63, loss: 0.1338, val_loss: 0.1226, acc: 0.971\n",
            "Epoch 64, loss: 0.1316, val_loss: 0.1200, acc: 0.971\n",
            "Epoch 65, loss: 0.1295, val_loss: 0.1176, acc: 0.971\n",
            "Epoch 66, loss: 0.1275, val_loss: 0.1153, acc: 0.971\n",
            "Epoch 67, loss: 0.1255, val_loss: 0.1130, acc: 0.986\n",
            "Epoch 68, loss: 0.1236, val_loss: 0.1107, acc: 0.986\n",
            "Epoch 69, loss: 0.1217, val_loss: 0.1084, acc: 0.986\n",
            "Epoch 70, loss: 0.1200, val_loss: 0.1063, acc: 0.986\n",
            "Epoch 71, loss: 0.1182, val_loss: 0.1035, acc: 0.986\n",
            "Epoch 72, loss: 0.1166, val_loss: 0.1011, acc: 0.986\n",
            "Epoch 73, loss: 0.1152, val_loss: 0.0997, acc: 0.986\n",
            "Epoch 74, loss: 0.1132, val_loss: 0.0978, acc: 0.986\n",
            "Epoch 75, loss: 0.1118, val_loss: 0.0958, acc: 0.986\n",
            "Epoch 76, loss: 0.1105, val_loss: 0.0941, acc: 0.986\n",
            "Epoch 77, loss: 0.1089, val_loss: 0.0923, acc: 0.986\n",
            "Epoch 78, loss: 0.1076, val_loss: 0.0907, acc: 0.986\n",
            "Epoch 79, loss: 0.1063, val_loss: 0.0892, acc: 0.986\n",
            "Epoch 80, loss: 0.1050, val_loss: 0.0878, acc: 0.986\n",
            "Epoch 81, loss: 0.1037, val_loss: 0.0864, acc: 0.986\n",
            "Epoch 82, loss: 0.1025, val_loss: 0.0850, acc: 0.986\n",
            "Epoch 83, loss: 0.1013, val_loss: 0.0837, acc: 0.986\n",
            "Epoch 84, loss: 0.1002, val_loss: 0.0822, acc: 0.986\n",
            "Epoch 85, loss: 0.0991, val_loss: 0.0809, acc: 0.986\n",
            "Epoch 86, loss: 0.0981, val_loss: 0.0799, acc: 0.986\n",
            "Epoch 87, loss: 0.0969, val_loss: 0.0787, acc: 0.986\n",
            "Epoch 88, loss: 0.0959, val_loss: 0.0774, acc: 0.986\n",
            "Epoch 89, loss: 0.0950, val_loss: 0.0763, acc: 0.986\n",
            "Epoch 90, loss: 0.0940, val_loss: 0.0752, acc: 0.986\n",
            "Epoch 91, loss: 0.0930, val_loss: 0.0742, acc: 0.986\n",
            "Epoch 92, loss: 0.0921, val_loss: 0.0732, acc: 0.986\n",
            "Epoch 93, loss: 0.0912, val_loss: 0.0720, acc: 0.986\n",
            "Epoch 94, loss: 0.0904, val_loss: 0.0711, acc: 0.986\n",
            "Epoch 95, loss: 0.0895, val_loss: 0.0702, acc: 0.986\n",
            "Epoch 96, loss: 0.0887, val_loss: 0.0693, acc: 0.986\n",
            "Epoch 97, loss: 0.0879, val_loss: 0.0683, acc: 0.986\n",
            "Epoch 98, loss: 0.0871, val_loss: 0.0675, acc: 0.986\n",
            "Epoch 99, loss: 0.0863, val_loss: 0.0666, acc: 0.986\n",
            "Epoch 100, loss: 0.0856, val_loss: 0.0659, acc: 0.986\n",
            "Test accuracy: 0.900\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Iris.csv')\n",
        "\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get mini-batches\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : ndarray, shape (n_samples,)\n",
        "      Labels\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      Random seed for NumPy\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_classes = 1\n",
        "\n",
        "class ExampleNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ExampleNet, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(n_classes, activation='sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "model = ExampleNet()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        history = model.fit(mini_batch_x, mini_batch_y, epochs=1, verbose=0)\n",
        "        total_loss += history.history['loss'][0]\n",
        "        total_acc += history.history['accuracy'][0]\n",
        "    total_loss /= len(get_mini_batch_train)\n",
        "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f\"Epoch {epoch+1}, loss: {total_loss:.4f}, val_loss: {val_loss:.4f}, acc: {total_acc/len(get_mini_batch_train):.3f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eELWOugMii1j"
      },
      "source": [
        "###Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqMlRjyjiKA4",
        "outputId": "4fefb6b8-b8ca-4473-d694-45b02345d9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss: 0.7909, val_loss: 0.7170, acc: 0.550\n",
            "Epoch 2, loss: 0.6806, val_loss: 0.6740, acc: 0.593\n",
            "Epoch 3, loss: 0.6604, val_loss: 0.6941, acc: 0.579\n",
            "Epoch 4, loss: 0.6345, val_loss: 0.6972, acc: 0.550\n",
            "Epoch 5, loss: 0.6090, val_loss: 0.6530, acc: 0.550\n",
            "Epoch 6, loss: 0.5883, val_loss: 0.6247, acc: 0.714\n",
            "Epoch 7, loss: 0.5737, val_loss: 0.6168, acc: 0.829\n",
            "Epoch 8, loss: 0.5577, val_loss: 0.6089, acc: 0.743\n",
            "Epoch 9, loss: 0.5406, val_loss: 0.5895, acc: 0.771\n",
            "Epoch 10, loss: 0.5245, val_loss: 0.5714, acc: 0.843\n",
            "Epoch 11, loss: 0.5103, val_loss: 0.5587, acc: 0.871\n",
            "Epoch 12, loss: 0.4959, val_loss: 0.5460, acc: 0.871\n",
            "Epoch 13, loss: 0.4811, val_loss: 0.5311, acc: 0.900\n",
            "Epoch 14, loss: 0.4673, val_loss: 0.5170, acc: 0.914\n",
            "Epoch 15, loss: 0.4540, val_loss: 0.5041, acc: 0.914\n",
            "Epoch 16, loss: 0.4407, val_loss: 0.4908, acc: 0.914\n",
            "Epoch 17, loss: 0.4276, val_loss: 0.4774, acc: 0.914\n",
            "Epoch 18, loss: 0.4150, val_loss: 0.4647, acc: 0.929\n",
            "Epoch 19, loss: 0.4025, val_loss: 0.4521, acc: 0.929\n",
            "Epoch 20, loss: 0.3903, val_loss: 0.4396, acc: 0.929\n",
            "Epoch 21, loss: 0.3786, val_loss: 0.4275, acc: 0.929\n",
            "Epoch 22, loss: 0.3671, val_loss: 0.4159, acc: 0.929\n",
            "Epoch 23, loss: 0.3560, val_loss: 0.4045, acc: 0.929\n",
            "Epoch 24, loss: 0.3451, val_loss: 0.3933, acc: 0.929\n",
            "Epoch 25, loss: 0.3347, val_loss: 0.3825, acc: 0.943\n",
            "Epoch 26, loss: 0.3246, val_loss: 0.3721, acc: 0.957\n",
            "Epoch 27, loss: 0.3148, val_loss: 0.3620, acc: 0.957\n",
            "Epoch 28, loss: 0.3054, val_loss: 0.3521, acc: 0.957\n",
            "Epoch 29, loss: 0.2963, val_loss: 0.3427, acc: 0.957\n",
            "Epoch 30, loss: 0.2876, val_loss: 0.3336, acc: 0.957\n",
            "Epoch 31, loss: 0.2792, val_loss: 0.3248, acc: 0.957\n",
            "Epoch 32, loss: 0.2711, val_loss: 0.3164, acc: 0.971\n",
            "Epoch 33, loss: 0.2634, val_loss: 0.3083, acc: 0.971\n",
            "Epoch 34, loss: 0.2559, val_loss: 0.3004, acc: 0.971\n",
            "Epoch 35, loss: 0.2489, val_loss: 0.2929, acc: 0.971\n",
            "Epoch 36, loss: 0.2421, val_loss: 0.2857, acc: 0.971\n",
            "Epoch 37, loss: 0.2355, val_loss: 0.2788, acc: 0.971\n",
            "Epoch 38, loss: 0.2293, val_loss: 0.2722, acc: 0.971\n",
            "Epoch 39, loss: 0.2234, val_loss: 0.2658, acc: 0.971\n",
            "Epoch 40, loss: 0.2177, val_loss: 0.2597, acc: 0.971\n",
            "Epoch 41, loss: 0.2122, val_loss: 0.2538, acc: 0.971\n",
            "Epoch 42, loss: 0.2070, val_loss: 0.2482, acc: 0.971\n",
            "Epoch 43, loss: 0.2020, val_loss: 0.2428, acc: 0.971\n",
            "Epoch 44, loss: 0.1973, val_loss: 0.2376, acc: 0.971\n",
            "Epoch 45, loss: 0.1927, val_loss: 0.2326, acc: 0.971\n",
            "Epoch 46, loss: 0.1884, val_loss: 0.2279, acc: 0.971\n",
            "Epoch 47, loss: 0.1842, val_loss: 0.2233, acc: 0.971\n",
            "Epoch 48, loss: 0.1802, val_loss: 0.2189, acc: 0.971\n",
            "Epoch 49, loss: 0.1764, val_loss: 0.2146, acc: 0.971\n",
            "Epoch 50, loss: 0.1728, val_loss: 0.2106, acc: 0.971\n",
            "Epoch 51, loss: 0.1693, val_loss: 0.2067, acc: 0.971\n",
            "Epoch 52, loss: 0.1659, val_loss: 0.2029, acc: 0.971\n",
            "Epoch 53, loss: 0.1627, val_loss: 0.1993, acc: 0.971\n",
            "Epoch 54, loss: 0.1596, val_loss: 0.1958, acc: 0.971\n",
            "Epoch 55, loss: 0.1567, val_loss: 0.1924, acc: 0.971\n",
            "Epoch 56, loss: 0.1538, val_loss: 0.1892, acc: 0.971\n",
            "Epoch 57, loss: 0.1511, val_loss: 0.1861, acc: 0.971\n",
            "Epoch 58, loss: 0.1485, val_loss: 0.1831, acc: 0.971\n",
            "Epoch 59, loss: 0.1460, val_loss: 0.1802, acc: 0.971\n",
            "Epoch 60, loss: 0.1436, val_loss: 0.1774, acc: 0.971\n",
            "Epoch 61, loss: 0.1412, val_loss: 0.1747, acc: 0.971\n",
            "Epoch 62, loss: 0.1390, val_loss: 0.1721, acc: 0.971\n",
            "Epoch 63, loss: 0.1368, val_loss: 0.1695, acc: 0.971\n",
            "Epoch 64, loss: 0.1347, val_loss: 0.1671, acc: 0.971\n",
            "Epoch 65, loss: 0.1327, val_loss: 0.1647, acc: 0.971\n",
            "Epoch 66, loss: 0.1308, val_loss: 0.1624, acc: 0.971\n",
            "Epoch 67, loss: 0.1289, val_loss: 0.1602, acc: 0.971\n",
            "Epoch 68, loss: 0.1271, val_loss: 0.1581, acc: 0.971\n",
            "Epoch 69, loss: 0.1253, val_loss: 0.1560, acc: 0.971\n",
            "Epoch 70, loss: 0.1236, val_loss: 0.1539, acc: 0.971\n",
            "Epoch 71, loss: 0.1220, val_loss: 0.1520, acc: 0.971\n",
            "Epoch 72, loss: 0.1204, val_loss: 0.1501, acc: 0.971\n",
            "Epoch 73, loss: 0.1189, val_loss: 0.1482, acc: 0.971\n",
            "Epoch 74, loss: 0.1174, val_loss: 0.1464, acc: 0.971\n",
            "Epoch 75, loss: 0.1159, val_loss: 0.1447, acc: 0.971\n",
            "Epoch 76, loss: 0.1145, val_loss: 0.1430, acc: 0.971\n",
            "Epoch 77, loss: 0.1132, val_loss: 0.1413, acc: 0.971\n",
            "Epoch 78, loss: 0.1119, val_loss: 0.1397, acc: 0.971\n",
            "Epoch 79, loss: 0.1106, val_loss: 0.1381, acc: 0.971\n",
            "Epoch 80, loss: 0.1093, val_loss: 0.1366, acc: 0.971\n",
            "Epoch 81, loss: 0.1081, val_loss: 0.1351, acc: 0.971\n",
            "Epoch 82, loss: 0.1069, val_loss: 0.1337, acc: 0.971\n",
            "Epoch 83, loss: 0.1058, val_loss: 0.1323, acc: 0.971\n",
            "Epoch 84, loss: 0.1047, val_loss: 0.1309, acc: 0.971\n",
            "Epoch 85, loss: 0.1036, val_loss: 0.1295, acc: 0.971\n",
            "Epoch 86, loss: 0.1026, val_loss: 0.1282, acc: 0.971\n",
            "Epoch 87, loss: 0.1015, val_loss: 0.1269, acc: 0.971\n",
            "Epoch 88, loss: 0.1005, val_loss: 0.1256, acc: 0.971\n",
            "Epoch 89, loss: 0.0996, val_loss: 0.1244, acc: 0.971\n",
            "Epoch 90, loss: 0.0986, val_loss: 0.1232, acc: 0.971\n",
            "Epoch 91, loss: 0.0977, val_loss: 0.1221, acc: 0.971\n",
            "Epoch 92, loss: 0.0968, val_loss: 0.1209, acc: 0.971\n",
            "Epoch 93, loss: 0.0959, val_loss: 0.1198, acc: 0.971\n",
            "Epoch 94, loss: 0.0950, val_loss: 0.1187, acc: 0.971\n",
            "Epoch 95, loss: 0.0942, val_loss: 0.1176, acc: 0.971\n",
            "Epoch 96, loss: 0.0934, val_loss: 0.1166, acc: 0.971\n",
            "Epoch 97, loss: 0.0926, val_loss: 0.1155, acc: 0.971\n",
            "Epoch 98, loss: 0.0918, val_loss: 0.1145, acc: 0.971\n",
            "Epoch 99, loss: 0.0910, val_loss: 0.1135, acc: 0.971\n",
            "Epoch 100, loss: 0.0903, val_loss: 0.1126, acc: 0.971\n",
            "Test accuracy: 0.900\n"
          ]
        }
      ],
      "source": [
        "class ExampleNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ExampleNet, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(n_classes, activation='sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "model = ExampleNet()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        history = model.fit(mini_batch_x, mini_batch_y, epochs=1, verbose=0)\n",
        "        total_loss += history.history['loss'][0]\n",
        "        total_acc += history.history['accuracy'][0]\n",
        "    total_loss /= len(get_mini_batch_train)\n",
        "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f\"Epoch {epoch+1}, loss: {total_loss:.4f}, val_loss: {val_loss:.4f}, acc: {total_acc/len(get_mini_batch_train):.3f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 4"
      ],
      "metadata": {
        "id": "1in1o5uYMck5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqyOu-_-jA3g",
        "outputId": "2a7fd598-8989-439b-acdb-03ec43413f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "94/94 [==============================] - 2s 7ms/step - loss: 86.1467 - root_mean_squared_error: 9.2815 - val_loss: 6.3260 - val_root_mean_squared_error: 2.5152\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 2.6298 - root_mean_squared_error: 1.6217 - val_loss: 1.5294 - val_root_mean_squared_error: 1.2367\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 1.2800 - root_mean_squared_error: 1.1314 - val_loss: 0.7444 - val_root_mean_squared_error: 0.8628\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.5650 - root_mean_squared_error: 0.7517 - val_loss: 0.3400 - val_root_mean_squared_error: 0.5831\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.2412 - root_mean_squared_error: 0.4911 - val_loss: 0.1433 - val_root_mean_squared_error: 0.3785\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1127 - root_mean_squared_error: 0.3358 - val_loss: 0.0738 - val_root_mean_squared_error: 0.2716\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0700 - root_mean_squared_error: 0.2646 - val_loss: 0.0528 - val_root_mean_squared_error: 0.2297\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0557 - root_mean_squared_error: 0.2360 - val_loss: 0.0450 - val_root_mean_squared_error: 0.2122\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0548 - root_mean_squared_error: 0.2341 - val_loss: 0.0457 - val_root_mean_squared_error: 0.2137\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0514 - root_mean_squared_error: 0.2268 - val_loss: 0.0442 - val_root_mean_squared_error: 0.2102\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0498 - root_mean_squared_error: 0.2231 - val_loss: 0.0463 - val_root_mean_squared_error: 0.2152\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0493 - root_mean_squared_error: 0.2220 - val_loss: 0.0495 - val_root_mean_squared_error: 0.2225\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0504 - root_mean_squared_error: 0.2245 - val_loss: 0.0426 - val_root_mean_squared_error: 0.2064\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0479 - root_mean_squared_error: 0.2189 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2060\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0482 - root_mean_squared_error: 0.2195 - val_loss: 0.0437 - val_root_mean_squared_error: 0.2090\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0483 - root_mean_squared_error: 0.2199 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0484 - root_mean_squared_error: 0.2200 - val_loss: 0.0448 - val_root_mean_squared_error: 0.2116\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0474 - root_mean_squared_error: 0.2177 - val_loss: 0.0426 - val_root_mean_squared_error: 0.2065\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0473 - root_mean_squared_error: 0.2174 - val_loss: 0.0416 - val_root_mean_squared_error: 0.2041\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0470 - root_mean_squared_error: 0.2169 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2046\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0483 - root_mean_squared_error: 0.2197 - val_loss: 0.0420 - val_root_mean_squared_error: 0.2048\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0476 - root_mean_squared_error: 0.2182 - val_loss: 0.0432 - val_root_mean_squared_error: 0.2078\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0475 - root_mean_squared_error: 0.2179 - val_loss: 0.0525 - val_root_mean_squared_error: 0.2292\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0463 - root_mean_squared_error: 0.2152 - val_loss: 0.0455 - val_root_mean_squared_error: 0.2133\n",
            "Epoch 25/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0494 - root_mean_squared_error: 0.2222 - val_loss: 0.0457 - val_root_mean_squared_error: 0.2138\n",
            "Epoch 26/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0502 - root_mean_squared_error: 0.2240 - val_loss: 0.0659 - val_root_mean_squared_error: 0.2567\n",
            "Epoch 27/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0519 - root_mean_squared_error: 0.2278 - val_loss: 0.0482 - val_root_mean_squared_error: 0.2195\n",
            "Epoch 28/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0480 - root_mean_squared_error: 0.2191 - val_loss: 0.0429 - val_root_mean_squared_error: 0.2071\n",
            "Epoch 29/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0496 - root_mean_squared_error: 0.2228 - val_loss: 0.0521 - val_root_mean_squared_error: 0.2282\n",
            "Epoch 30/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0501 - root_mean_squared_error: 0.2238 - val_loss: 0.0429 - val_root_mean_squared_error: 0.2071\n",
            "Epoch 31/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0480 - root_mean_squared_error: 0.2191 - val_loss: 0.0414 - val_root_mean_squared_error: 0.2035\n",
            "Epoch 32/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0486 - root_mean_squared_error: 0.2204 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2061\n",
            "Epoch 33/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0484 - root_mean_squared_error: 0.2201 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2053\n",
            "Epoch 34/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0478 - root_mean_squared_error: 0.2187 - val_loss: 0.0455 - val_root_mean_squared_error: 0.2133\n",
            "Epoch 35/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0489 - root_mean_squared_error: 0.2210 - val_loss: 0.0432 - val_root_mean_squared_error: 0.2078\n",
            "Epoch 36/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0532 - root_mean_squared_error: 0.2307 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044\n",
            "Epoch 37/100\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0503 - root_mean_squared_error: 0.2244 - val_loss: 0.0489 - val_root_mean_squared_error: 0.2211\n",
            "Epoch 38/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0493 - root_mean_squared_error: 0.2219 - val_loss: 0.0458 - val_root_mean_squared_error: 0.2141\n",
            "Epoch 39/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0475 - root_mean_squared_error: 0.2179 - val_loss: 0.0526 - val_root_mean_squared_error: 0.2293\n",
            "Epoch 40/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0524 - root_mean_squared_error: 0.2289 - val_loss: 0.0416 - val_root_mean_squared_error: 0.2039\n",
            "Epoch 41/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0488 - root_mean_squared_error: 0.2209 - val_loss: 0.0514 - val_root_mean_squared_error: 0.2266\n",
            "Epoch 42/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0540 - root_mean_squared_error: 0.2323 - val_loss: 0.0561 - val_root_mean_squared_error: 0.2369\n",
            "Epoch 43/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0478 - root_mean_squared_error: 0.2187 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2058\n",
            "Epoch 44/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0510 - root_mean_squared_error: 0.2259 - val_loss: 0.0566 - val_root_mean_squared_error: 0.2378\n",
            "Epoch 45/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0491 - root_mean_squared_error: 0.2215 - val_loss: 0.0431 - val_root_mean_squared_error: 0.2076\n",
            "Epoch 46/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0524 - root_mean_squared_error: 0.2289 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
            "Epoch 47/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0503 - root_mean_squared_error: 0.2243 - val_loss: 0.0429 - val_root_mean_squared_error: 0.2072\n",
            "Epoch 48/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0493 - root_mean_squared_error: 0.2220 - val_loss: 0.0530 - val_root_mean_squared_error: 0.2303\n",
            "Epoch 49/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0492 - root_mean_squared_error: 0.2217 - val_loss: 0.0429 - val_root_mean_squared_error: 0.2070\n",
            "Epoch 50/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0498 - root_mean_squared_error: 0.2233 - val_loss: 0.0589 - val_root_mean_squared_error: 0.2428\n",
            "Epoch 51/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0485 - root_mean_squared_error: 0.2203 - val_loss: 0.0435 - val_root_mean_squared_error: 0.2085\n",
            "Epoch 52/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0477 - root_mean_squared_error: 0.2184 - val_loss: 0.0548 - val_root_mean_squared_error: 0.2340\n",
            "Epoch 53/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0480 - root_mean_squared_error: 0.2191 - val_loss: 0.0543 - val_root_mean_squared_error: 0.2329\n",
            "Epoch 54/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0575 - root_mean_squared_error: 0.2397 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2052\n",
            "Epoch 55/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0495 - root_mean_squared_error: 0.2225 - val_loss: 0.0428 - val_root_mean_squared_error: 0.2068\n",
            "Epoch 56/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0536 - root_mean_squared_error: 0.2315 - val_loss: 0.0416 - val_root_mean_squared_error: 0.2040\n",
            "Epoch 57/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0495 - root_mean_squared_error: 0.2224 - val_loss: 0.0524 - val_root_mean_squared_error: 0.2288\n",
            "Epoch 58/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0502 - root_mean_squared_error: 0.2241 - val_loss: 0.0438 - val_root_mean_squared_error: 0.2094\n",
            "Epoch 59/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0499 - root_mean_squared_error: 0.2233 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2063\n",
            "Epoch 60/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0499 - root_mean_squared_error: 0.2233 - val_loss: 0.0416 - val_root_mean_squared_error: 0.2040\n",
            "Epoch 61/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0527 - root_mean_squared_error: 0.2296 - val_loss: 0.0524 - val_root_mean_squared_error: 0.2288\n",
            "Epoch 62/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0503 - root_mean_squared_error: 0.2243 - val_loss: 0.0468 - val_root_mean_squared_error: 0.2164\n",
            "Epoch 63/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0504 - root_mean_squared_error: 0.2245 - val_loss: 0.0445 - val_root_mean_squared_error: 0.2110\n",
            "Epoch 64/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0477 - root_mean_squared_error: 0.2184 - val_loss: 0.0415 - val_root_mean_squared_error: 0.2037\n",
            "Epoch 65/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0514 - root_mean_squared_error: 0.2267 - val_loss: 0.0631 - val_root_mean_squared_error: 0.2511\n",
            "Epoch 66/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0494 - root_mean_squared_error: 0.2222 - val_loss: 0.0634 - val_root_mean_squared_error: 0.2519\n",
            "Epoch 67/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0518 - root_mean_squared_error: 0.2276 - val_loss: 0.0427 - val_root_mean_squared_error: 0.2066\n",
            "Epoch 68/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0527 - root_mean_squared_error: 0.2296 - val_loss: 0.0540 - val_root_mean_squared_error: 0.2324\n",
            "Epoch 69/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0506 - root_mean_squared_error: 0.2249 - val_loss: 0.0437 - val_root_mean_squared_error: 0.2091\n",
            "Epoch 70/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0499 - root_mean_squared_error: 0.2235 - val_loss: 0.0445 - val_root_mean_squared_error: 0.2110\n",
            "Epoch 71/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0492 - root_mean_squared_error: 0.2218 - val_loss: 0.0415 - val_root_mean_squared_error: 0.2038\n",
            "Epoch 72/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0507 - root_mean_squared_error: 0.2251 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2062\n",
            "Epoch 73/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0476 - root_mean_squared_error: 0.2183 - val_loss: 0.0421 - val_root_mean_squared_error: 0.2051\n",
            "Epoch 74/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0505 - root_mean_squared_error: 0.2247 - val_loss: 0.0424 - val_root_mean_squared_error: 0.2060\n",
            "Epoch 75/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0484 - root_mean_squared_error: 0.2201 - val_loss: 0.0489 - val_root_mean_squared_error: 0.2211\n",
            "Epoch 76/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0492 - root_mean_squared_error: 0.2219 - val_loss: 0.0556 - val_root_mean_squared_error: 0.2357\n",
            "Epoch 77/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0542 - root_mean_squared_error: 0.2329 - val_loss: 0.0418 - val_root_mean_squared_error: 0.2044\n",
            "Epoch 78/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0538 - root_mean_squared_error: 0.2321 - val_loss: 0.0433 - val_root_mean_squared_error: 0.2082\n",
            "Epoch 79/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0539 - root_mean_squared_error: 0.2321 - val_loss: 0.0511 - val_root_mean_squared_error: 0.2261\n",
            "Epoch 80/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0496 - root_mean_squared_error: 0.2227 - val_loss: 0.0531 - val_root_mean_squared_error: 0.2304\n",
            "Epoch 81/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0547 - root_mean_squared_error: 0.2340 - val_loss: 0.0697 - val_root_mean_squared_error: 0.2641\n",
            "Epoch 82/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0511 - root_mean_squared_error: 0.2259 - val_loss: 0.0702 - val_root_mean_squared_error: 0.2649\n",
            "Epoch 83/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0562 - root_mean_squared_error: 0.2371 - val_loss: 0.0425 - val_root_mean_squared_error: 0.2061\n",
            "Epoch 84/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0476 - root_mean_squared_error: 0.2182 - val_loss: 0.0510 - val_root_mean_squared_error: 0.2259\n",
            "Epoch 85/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0519 - root_mean_squared_error: 0.2279 - val_loss: 0.0419 - val_root_mean_squared_error: 0.2046\n",
            "Epoch 86/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0517 - root_mean_squared_error: 0.2273 - val_loss: 0.0518 - val_root_mean_squared_error: 0.2276\n",
            "Epoch 87/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0529 - root_mean_squared_error: 0.2300 - val_loss: 0.0547 - val_root_mean_squared_error: 0.2338\n",
            "Epoch 88/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0479 - root_mean_squared_error: 0.2189 - val_loss: 0.0478 - val_root_mean_squared_error: 0.2185\n",
            "Epoch 89/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0485 - root_mean_squared_error: 0.2202 - val_loss: 0.0423 - val_root_mean_squared_error: 0.2057\n",
            "Epoch 90/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0513 - root_mean_squared_error: 0.2266 - val_loss: 0.0653 - val_root_mean_squared_error: 0.2556\n",
            "Epoch 91/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0488 - root_mean_squared_error: 0.2210 - val_loss: 0.0738 - val_root_mean_squared_error: 0.2716\n",
            "Epoch 92/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0543 - root_mean_squared_error: 0.2330 - val_loss: 0.0466 - val_root_mean_squared_error: 0.2159\n",
            "Epoch 93/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.0522 - root_mean_squared_error: 0.2285 - val_loss: 0.0411 - val_root_mean_squared_error: 0.2027\n",
            "Epoch 94/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0469 - root_mean_squared_error: 0.2165 - val_loss: 0.0422 - val_root_mean_squared_error: 0.2053\n",
            "Epoch 95/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0472 - root_mean_squared_error: 0.2173 - val_loss: 0.0414 - val_root_mean_squared_error: 0.2034\n",
            "Epoch 96/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0463 - root_mean_squared_error: 0.2151 - val_loss: 0.0415 - val_root_mean_squared_error: 0.2037\n",
            "Epoch 97/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0502 - root_mean_squared_error: 0.2241 - val_loss: 0.0606 - val_root_mean_squared_error: 0.2461\n",
            "Epoch 98/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0481 - root_mean_squared_error: 0.2193 - val_loss: 0.0482 - val_root_mean_squared_error: 0.2195\n",
            "Epoch 99/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0510 - root_mean_squared_error: 0.2258 - val_loss: 0.0433 - val_root_mean_squared_error: 0.2080\n",
            "Epoch 100/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0496 - root_mean_squared_error: 0.2228 - val_loss: 0.0432 - val_root_mean_squared_error: 0.2078\n",
            "10/10 - 0s - loss: 0.0485 - root_mean_squared_error: 0.2202 - 70ms/epoch - 7ms/step\n",
            "Test RMSE: 0.220\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y).astype(np.float32)\n",
        "\n",
        "y = np.log(y).reshape(-1, 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_classes = 1\n",
        "\n",
        "# Define the model\n",
        "class HousePriceModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(HousePriceModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(n_classes)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = HousePriceModel()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_rmse = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test RMSE: {test_rmse:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float32) / 255\n",
        "X_test = X_test.astype(np.float32) / 255\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train = y_train[:, np.newaxis]\n",
        "y_test = y_test[:, np.newaxis]\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "\n",
        "n_hidden1 = 256\n",
        "n_hidden2 = 256\n",
        "n_input = 784\n",
        "n_classes = 10\n",
        "\n",
        "class MNISTModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(n_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "model = MNISTModel()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot, verbose=2)\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JctvDsptSNIW",
        "outputId": "2c68d226-2cca-4c95-f8cf-e0e42bd7b19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "375/375 [==============================] - 11s 22ms/step - loss: 0.2900 - accuracy: 0.9174 - val_loss: 0.1302 - val_accuracy: 0.9607\n",
            "Epoch 2/20\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.1068 - accuracy: 0.9680 - val_loss: 0.1026 - val_accuracy: 0.9692\n",
            "Epoch 3/20\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0709 - accuracy: 0.9784 - val_loss: 0.0915 - val_accuracy: 0.9715\n",
            "Epoch 4/20\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0498 - accuracy: 0.9845 - val_loss: 0.0838 - val_accuracy: 0.9753\n",
            "Epoch 5/20\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0361 - accuracy: 0.9888 - val_loss: 0.0789 - val_accuracy: 0.9762\n",
            "Epoch 6/20\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0273 - accuracy: 0.9910 - val_loss: 0.0805 - val_accuracy: 0.9772\n",
            "Epoch 7/20\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0774 - val_accuracy: 0.9779\n",
            "Epoch 8/20\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.0787 - val_accuracy: 0.9785\n",
            "Epoch 9/20\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0915 - val_accuracy: 0.9792\n",
            "Epoch 10/20\n",
            "375/375 [==============================] - 4s 9ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.0788 - val_accuracy: 0.9801\n",
            "Epoch 11/20\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.0127 - accuracy: 0.9958 - val_loss: 0.1004 - val_accuracy: 0.9771\n",
            "Epoch 12/20\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.1057 - val_accuracy: 0.9757\n",
            "Epoch 13/20\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.0968 - val_accuracy: 0.9791\n",
            "Epoch 14/20\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0992 - val_accuracy: 0.9795\n",
            "Epoch 15/20\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.1117 - val_accuracy: 0.9759\n",
            "Epoch 16/20\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.1002 - val_accuracy: 0.9787\n",
            "Epoch 17/20\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 0.0982 - val_accuracy: 0.9801\n",
            "Epoch 18/20\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.1018 - val_accuracy: 0.9777\n",
            "Epoch 19/20\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.1122 - val_accuracy: 0.9802\n",
            "Epoch 20/20\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.1024 - val_accuracy: 0.9798\n",
            "313/313 - 1s - loss: 0.1123 - accuracy: 0.9769 - 629ms/epoch - 2ms/step\n",
            "Test Accuracy: 0.977\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}