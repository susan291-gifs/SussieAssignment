{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susan291-gifs/SussieAssignment/blob/main/SimpleConv1d_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtF4hUQneXbc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pRblRUlnTVW"
      },
      "source": [
        "###Problem 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejr4vQY9epTz"
      },
      "outputs": [],
      "source": [
        "class SimpleInitializerConv2d:\n",
        "    def __init__(self, sigma=0.01):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, F, C, FH, FW):\n",
        "        return self.sigma * np.random.randn(F, C, FH, FW)\n",
        "\n",
        "    def B(self, F):\n",
        "        return np.zeros(F)\n",
        "\n",
        "class Conv2d:\n",
        "    def __init__(self, F, C, FH, FW, P, S, initializer=None, optimizer=None, activation=None):\n",
        "        self.P = P\n",
        "        self.S = S\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        self.W = self.initializer.W(F, C, FH, FW)\n",
        "        self.B = self.initializer.B(F)\n",
        "\n",
        "    def set_weights(self, W, B):\n",
        "        self.W = W\n",
        "        self.B = B\n",
        "\n",
        "    def output_shape2d(self, H, W, pad_h, pad_w, FH, FW, S_h, S_w):\n",
        "        OH = (H + 2 * pad_h - FH) // S_h + 1\n",
        "        OW = (W + 2 * pad_w - FW) // S_w + 1\n",
        "        return OH, OW\n",
        "\n",
        "    def forward(self, X, debug=False):\n",
        "        self.X = X\n",
        "        N, C, H, W = self.X.shape\n",
        "        F, C, FH, FW = self.W.shape\n",
        "        OH, OW = self.output_shape2d(H, W, self.P, self.P, FH, FW, self.S, self.S)\n",
        "        self.params = N, C, H, W, F, FH, FW, OH, OW\n",
        "        A = np.zeros([N, F, OH, OW])\n",
        "        self.X_pad = np.pad(self.X, ((0, 0), (0, 0), (self.P, self.P), (self.P, self.P)))\n",
        "\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(OH):\n",
        "                    for col in range(OW):\n",
        "                        A[n, ch, row, col] = np.sum(self.X_pad[n, :, row*self.S:row*self.S+FH, col*self.S:col*self.S+FW] * self.W[ch, :, :, :]) + self.B[ch]\n",
        "\n",
        "        if debug:\n",
        "            return A\n",
        "        else:\n",
        "            return self.activation.forward(A) if self.activation else A\n",
        "\n",
        "    def backward(self, dZ, debug=False):\n",
        "        if debug:\n",
        "            dA = dZ\n",
        "        else:\n",
        "            dA = self.activation.backward(dZ) if self.activation else dZ\n",
        "        N, C, H, W, F, FH, FW, OH, OW = self.params\n",
        "        dX = np.zeros(self.X_pad.shape)\n",
        "        self.dW = np.zeros(self.W.shape)\n",
        "        self.dB = np.zeros(self.B.shape)\n",
        "\n",
        "        for n in range(N):\n",
        "            for ch in range(F):\n",
        "                for row in range(OH):\n",
        "                    for col in range(OW):\n",
        "                        dX[n, :, row*self.S:row*self.S+FH, col*self.S:col*self.S+FW] += dA[n, ch, row, col] * self.W[ch, :, :, :]\n",
        "                        self.dW[ch, :, :, :] += dA[n, ch, row, col] * self.X_pad[n, :, row*self.S:row*self.S+FH, col*self.S:col*self.S+FW]\n",
        "\n",
        "        if self.P > 0:\n",
        "            dX = dX[:, :, self.P:-self.P, self.P:-self.P]\n",
        "\n",
        "        for ch in range(F):\n",
        "            self.dB[ch] = np.sum(dA[:, ch, :, :])\n",
        "\n",
        "        self.optimizer.update(self)\n",
        "        return dX\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.maximum(0, A)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (self.A > 0)\n",
        "\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxrmyQjxospj"
      },
      "source": [
        "###Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3c-ZEtFnabU",
        "outputId": "705d2c96-969f-4449-9b59-90a090e8d1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Output:\n",
            " [[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "Backward Output (Gradient w.r.t input):\n",
            " [[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  13.  27. -11.]\n",
            "   [  0. -10. -11.   0.]]]]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[[[ 1,  2,  3,  4],\n",
        "                [ 5,  6,  7,  8],\n",
        "                [ 9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "\n",
        "w = np.array([[[[ 0.,  0.,  0.],\n",
        "                [ 0.,  1.,  0.],\n",
        "                [ 0., -1.,  0.]]],\n",
        "\n",
        "              [[[ 0.,  0.,  0.],\n",
        "                [ 0., -1.,  1.],\n",
        "                [ 0.,  0.,  0.]]]])\n",
        "\n",
        "b = np.array([0., 0.])\n",
        "\n",
        "\n",
        "conv_layer = Conv2d(F=2, C=1, FH=3, FW=3, P=0, S=1, initializer=SimpleInitializerConv2d(), optimizer=SGD(lr=0.01), activation=None)\n",
        "conv_layer.set_weights(w, b)\n",
        "\n",
        "\n",
        "output = conv_layer.forward(x)\n",
        "print(\"Forward Output:\\n\", output)\n",
        "\n",
        "\n",
        "delta = np.array([[[[ -4,  -4],\n",
        "                    [ 10,  11]],\n",
        "\n",
        "                   [[  1,  -7],\n",
        "                    [  1, -11]]]])\n",
        "\n",
        "\n",
        "dX = conv_layer.backward(delta)\n",
        "print(\"Backward Output (Gradient w.r.t input):\\n\", dX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHSSH3I40sC"
      },
      "source": [
        "###Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYmMqTu3oygo",
        "outputId": "d1ccadd9-b164-4492-a4a8-8ae1de310dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Height: 4, Output Width: 4\n"
          ]
        }
      ],
      "source": [
        "def calculate_output_size(Nh_in, Nw_in, Ph, Pw, Fh, Fw, Sh, Sw):\n",
        "    \"\"\"\n",
        "    Calculate the output size after 2D convolution.\n",
        "\n",
        "    Parameters:\n",
        "    - Nh_in (int): Input height\n",
        "    - Nw_in (int): Input width\n",
        "    - Ph (int): Padding height\n",
        "    - Pw (int): Padding width\n",
        "    - Fh (int): Filter height\n",
        "    - Fw (int): Filter width\n",
        "    - Sh (int): Stride height\n",
        "    - Sw (int): Stride width\n",
        "\n",
        "    Returns:\n",
        "    - Nh_out (int): Output height\n",
        "    - Nw_out (int): Output width\n",
        "    \"\"\"\n",
        "    Nh_out = math.floor((Nh_in + 2 * Ph - Fh) / Sh) + 1\n",
        "    Nw_out = math.floor((Nw_in + 2 * Pw - Fw) / Sw) + 1\n",
        "\n",
        "    return Nh_out, Nw_out\n",
        "\n",
        "# Example usage\n",
        "Nh_in = 6  # Input height\n",
        "Nw_in = 6  # Input width\n",
        "Ph = 0     # Padding height\n",
        "Pw = 0     # Padding width\n",
        "Fh = 3     # Filter height\n",
        "Fw = 3     # Filter width\n",
        "Sh = 1     # Stride height\n",
        "Sw = 1     # Stride width\n",
        "\n",
        "Nh_out, Nw_out = calculate_output_size(Nh_in, Nw_in, Ph, Pw, Fh, Fw, Sh, Sw)\n",
        "print(f\"Output Height: {Nh_out}, Output Width: {Nw_out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKa2oEEnPhQK"
      },
      "source": [
        "###Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGjnYNYH44hM",
        "outputId": "611b04c6-865b-4118-c099-e6456799aafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Output:\n",
            " [[[[ 6.  8.]\n",
            "   [14. 16.]]]]\n",
            "Backward Output (Gradient w.r.t input):\n",
            " [[[[0 0 0 0]\n",
            "   [0 1 0 2]\n",
            "   [0 0 0 0]\n",
            "   [0 3 0 4]]]]\n"
          ]
        }
      ],
      "source": [
        "class MaxPool2D:\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        N, C, H, W = X.shape\n",
        "        pool_height, pool_width = self.pool_size\n",
        "        stride_height, stride_width = self.stride\n",
        "\n",
        "        out_height = (H - pool_height) // stride_height + 1\n",
        "        out_width = (W - pool_width) // stride_width + 1\n",
        "\n",
        "        out = np.zeros((N, C, out_height, out_width))\n",
        "        self.max_indices = np.zeros((N, C, out_height, out_width, 2), dtype=int)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_height):\n",
        "                    for j in range(out_width):\n",
        "                        h_start = i * stride_height\n",
        "                        h_end = h_start + pool_height\n",
        "                        w_start = j * stride_width\n",
        "                        w_end = w_start + pool_width\n",
        "\n",
        "                        pool_region = X[n, c, h_start:h_end, w_start:w_end]\n",
        "                        out[n, c, i, j] = np.max(pool_region)\n",
        "                        max_index = np.unravel_index(np.argmax(pool_region), pool_region.shape)\n",
        "                        self.max_indices[n, c, i, j] = (h_start + max_index[0], w_start + max_index[1])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dA):\n",
        "        N, C, H, W = self.X.shape\n",
        "        pool_height, pool_width = self.pool_size\n",
        "        stride_height, stride_width = self.stride\n",
        "        _, _, out_height, out_width = dA.shape\n",
        "\n",
        "        dX = np.zeros_like(self.X)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_height):\n",
        "                    for j in range(out_width):\n",
        "                        max_h, max_w = self.max_indices[n, c, i, j]\n",
        "                        dX[n, c, max_h, max_w] += dA[n, c, i, j]\n",
        "\n",
        "        return dX\n",
        "\n",
        "\n",
        "x = np.array([[[[ 1,  2,  3,  4],\n",
        "                [ 5,  6,  7,  8],\n",
        "                [ 9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "\n",
        "maxpool = MaxPool2D(pool_size=(2, 2), stride=(2, 2))\n",
        "output = maxpool.forward(x)\n",
        "print(\"Forward Output:\\n\", output)\n",
        "\n",
        "\n",
        "delta = np.array([[[[ 1,  2],\n",
        "                    [ 3,  4]]]])\n",
        "\n",
        "dX = maxpool.backward(delta)\n",
        "print(\"Backward Output (Gradient w.r.t input):\\n\", dX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np0_D-lSf26u",
        "outputId": "2fb79425-3b16-4f4e-d22e-78b499d3e7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "844/844 [==============================] - 188s 212ms/step - loss: 0.1351 - accuracy: 0.9579 - val_loss: 0.0471 - val_accuracy: 0.9867\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 163s 194ms/step - loss: 0.0402 - accuracy: 0.9876 - val_loss: 0.0440 - val_accuracy: 0.9885\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 170s 201ms/step - loss: 0.0241 - accuracy: 0.9924 - val_loss: 0.0463 - val_accuracy: 0.9883\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 176s 209ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.0485 - val_accuracy: 0.9892\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 171s 202ms/step - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.0378 - val_accuracy: 0.9902\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 165s 195ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 0.0489 - val_accuracy: 0.9890\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 166s 197ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0581 - val_accuracy: 0.9872\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 167s 197ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0631 - val_accuracy: 0.9885\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 166s 196ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 0.0709 - val_accuracy: 0.9850\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 164s 195ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0550 - val_accuracy: 0.9900\n",
            "313/313 [==============================] - 9s 28ms/step - loss: 0.0502 - accuracy: 0.9886\n",
            "Test Accuracy: 0.9886000156402588\n"
          ]
        }
      ],
      "source": [
        "class FlattenLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.shape = X.shape\n",
        "        return X.reshape(len(X), -1)\n",
        "\n",
        "    def backward(self, X):\n",
        "        return X.reshape(self.shape)\n",
        "\n",
        "def load_mnist():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "    y_train = to_categorical(y_train, 10)\n",
        "    y_test = to_categorical(y_test, 10)\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "def build_model():\n",
        "    input_layer = Input(shape=(28, 28, 1))\n",
        "    x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "    x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    output_layer = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n",
        "\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}