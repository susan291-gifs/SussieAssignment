{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susan291-gifs/SussieAssignment/blob/main/LinearRegression_asspgnment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SKYUm8x-Rctg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 1"
      ],
      "metadata": {
        "id": "pLAsBk3KQJG4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLpqXl3UzrtX"
      },
      "outputs": [],
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      Number of iterations\n",
        "    lr : float\n",
        "      Learning rate\n",
        "    no_bias : bool\n",
        "      True if no bias term is included\n",
        "    verbose : bool\n",
        "      True to output the learning process\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : of the following form. ndarray, shape (n_features,)\n",
        "      Parameters\n",
        "    self.loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record losses on training data\n",
        "    self.val_loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record loss on validation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"\n",
        "        Compute a linear hypothetical function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "          Training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h_theta : of the following form. ndarray, shape (n_samples, 1)\n",
        "          Estimated result by linear hypothetical function\n",
        "\n",
        "        \"\"\"\n",
        "        if not self.no_bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        h_theta = np.dot(X, self.coef_)\n",
        "\n",
        "        return h_theta\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct answer value of training data\n",
        "        X_val : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of verification data\n",
        "        y_val : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct value of verification data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'Training on {X.shape[0]} samples')\n",
        "\n",
        "        if not self.no_bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        self.coef_ = np.random.randn(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            h_theta = self._linear_hypothesis(X)\n",
        "\n",
        "            error = h_theta - y\n",
        "\n",
        "            gradient = np.dot(X.T, error) / X.shape[0]\n",
        "\n",
        "            self.coef_ -= self.lr * gradient\n",
        "\n",
        "            self.loss[i] = np.mean((error) ** 2)\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                h_theta_val = self._linear_hypothesis(X_val)\n",
        "\n",
        "                error_val = h_theta_val - y_val\n",
        "\n",
        "                self.val_loss[i] = np.mean((error_val) ** 2)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f'Iteration {i+1}, Training Loss: {self.loss[i]}')\n",
        "                if X_val is not None and y_val is not None:\n",
        "                    print(f'Iteration {i+1}, Validation Loss: {self.val_loss[i]}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            of the following form. ndarray, shape (n_samples, 1)\n",
        "            Estimated result by linear regression\n",
        "        \"\"\"\n",
        "        h_theta = self._linear_hypothesis(X)\n",
        "\n",
        "        return h_theta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 2"
      ],
      "metadata": {
        "id": "uuRw_cCbQj3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      Number of iterations\n",
        "    lr : float\n",
        "      Learning rate\n",
        "    no_bias : bool\n",
        "      True if no bias term is included\n",
        "    verbose : bool\n",
        "      True to output the learning process\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : of the following form. ndarray, shape (n_features,)\n",
        "      Parameters\n",
        "    self.loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record losses on training data\n",
        "    self.val_loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record loss on validation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"\n",
        "        Compute a linear hypothetical function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "          Training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h_theta : of the following form. ndarray, shape (n_samples, 1)\n",
        "          Estimated result by linear hypothetical function\n",
        "\n",
        "        \"\"\"\n",
        "        if not self.no_bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        h_theta = np.dot(X, self.coef_)\n",
        "\n",
        "        return h_theta\n",
        "\n",
        "    def _gradient_descent(self, X, error):\n",
        "        \"\"\"\n",
        "        Update parameters using gradient descent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "          Training data\n",
        "        error : of the following form. ndarray, shape (n_samples, 1)\n",
        "          Error between predicted and actual values\n",
        "\n",
        "        \"\"\"\n",
        "        gradient = np.dot(X.T, error) / X.shape[0]\n",
        "\n",
        "        self.coef_ -= self.lr * gradient\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct answer value of training data\n",
        "        X_val : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of verification data\n",
        "        y_val : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct value of verification data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'Training on {X.shape[0]} samples')\n",
        "\n",
        "        if not self.no_bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        self.coef_ = np.random.randn(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            h_theta = self._linear_hypothesis(X)\n",
        "\n",
        "            error = h_theta - y\n",
        "\n",
        "            self._gradient_descent(X, error)\n",
        "\n",
        "            self.loss[i] = np.mean((error) ** 2)\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                h_theta_val = self._linear_hypothesis(X_val)\n",
        "\n",
        "                error_val = h_theta_val - y_val\n",
        "\n",
        "                self.val_loss[i] = np.mean((error_val) ** 2)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f'Iteration {i+1}, Training Loss: {self.loss[i]}')\n",
        "                if X_val is not None and y_val is not None:\n",
        "                    print(f'Iteration {i+1}, Validation Loss: {self.val_loss[i]}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            of the following form. ndarray, shape (n_samples, 1)\n",
        "            Estimated result by linear regression\n",
        "        \"\"\"\n",
        "        h_theta = self._linear_hypo\n"
      ],
      "metadata": {
        "id": "Qg5iYq9RQMiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 3"
      ],
      "metadata": {
        "id": "wX9--myiQhXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      Number of iterations\n",
        "    lr : float\n",
        "      Learning rate\n",
        "    no_bias : bool\n",
        "      True if no bias term is included\n",
        "    verbose : bool\n",
        "      True to output the learning process\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : of the following form. ndarray, shape (n_features,)\n",
        "      Parameters\n",
        "    self.loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record losses on training data\n",
        "    self.val_loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record loss on validation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"\n",
        "        Compute a linear hypothetical function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "          Training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h_theta : of the following form. ndarray, shape (n_samples, 1)\n",
        "          Estimated result by linear hypothetical function\n",
        "\n",
        "        \"\"\"\n",
        "        if not self.no_bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        h_theta = np.dot(X, self.coef_)\n",
        "\n",
        "        return h_theta\n",
        "\n",
        "    def _gradient_descent(self, X, error):\n",
        "        \"\"\"\n",
        "        Update parameters using gradient descent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "          Training data\n",
        "        error : of the following form. ndarray, shape (n_samples, 1)\n",
        "          Error between predicted and actual values\n",
        "\n",
        "        \"\"\"\n",
        "        gradient = np.dot(X.T, error) / X.shape[0]\n",
        "\n",
        "        self.coef_ -= self.lr * gradient\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct answer value of training data\n",
        "        X_val : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of verification data\n",
        "        y_val : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct value of verification data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'Training on {X.shape[0]} samples')\n",
        "\n",
        "        if not self.no_bias:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        self.coef_ = np.random.randn(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            h_theta = self._linear_hypothesis(X)\n",
        "\n",
        "            error = h_theta - y\n",
        "\n",
        "            self._gradient_descent(X, error)\n",
        "\n",
        "            self.loss[i] = np.mean((error) ** 2)\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                h_theta_val = self._linear_hypothesis(X_val)\n",
        "\n",
        "                error_val = h_theta_val - y_val\n",
        "\n",
        "                self.val_loss[i] = np.mean((error_val) ** 2)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f'Iteration {i+1}, Training Loss: {self.loss[i]}')\n",
        "                if X_val is not None and y_val is not None:\n",
        "                    print(f'Iteration {i+1}, Validation Loss: {self.val_loss[i]}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "        h_theta : of the following form. ndarray, shape (n_samples, 1)\n",
        "            Estimated result by linear regression\n",
        "        \"\"\"\n",
        "        h_theta = self._linear_hypothesis(X)\n",
        "        return h_theta\n"
      ],
      "metadata": {
        "id": "ciu1a1CLQfLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 4"
      ],
      "metadata": {
        "id": "v1ovm7AVQx80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y_pred, y):\n",
        "    \"\"\"\n",
        "    Calculation of mean square error\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_pred : of the following form. ndarray, shape (n_samples,)\n",
        "      Estimated value\n",
        "    y : of the following form. ndarray, shape (n_samples,)\n",
        "      Correct answer value\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    mse : numpy.float\n",
        "      Mean squared error\n",
        "    \"\"\"\n",
        "    mse = np.mean((y_pred - y) ** 2)\n",
        "    return mse\n"
      ],
      "metadata": {
        "id": "rDQcZGKXQxH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 5"
      ],
      "metadata": {
        "id": "FiAdzc3_RABY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_cost(self, X, y):\n",
        "    \"\"\"\n",
        "    Compute the cost function for linear regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : numpy.ndarray, shape (n_samples, n_features)\n",
        "        Features of training data.\n",
        "    y : numpy.ndarray, shape (n_samples,)\n",
        "        Correct answer value of training data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cost : numpy.float\n",
        "        Value of the cost function.\n",
        "    \"\"\"\n",
        "\n",
        "    m = len(y)\n",
        "\n",
        "\n",
        "    h_theta = self._linear_hypothesis(X)\n",
        "\n",
        "\n",
        "    cost = np.sum((h_theta - y) ** 2) / (2 * m)\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "6aj2tb3hQ_KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 6"
      ],
      "metadata": {
        "id": "S46qo4SmSIwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')\n",
        "\n",
        "\n",
        "X = train[['GrLivArea', 'YearBuilt']].values\n",
        "y = train['SalePrice'].values\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XZyYbkkVRfLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    def __init__(self, num_iter=1000, lr=0.01):\n",
        "        self.num_iter = num_iter\n",
        "        self.lr = lr\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        return np.dot(X, self.theta)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        m = len(y)\n",
        "\n",
        "        for _ in range(self.num_iter):\n",
        "            error = self._linear_hypothesis(X) - y\n",
        "            gradient = np.dot(X.T, error) / m\n",
        "            self.theta -= self.lr * gradient\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self._linear_hypothesis(X)\n",
        "\n",
        "scratch_lr = ScratchLinearRegression()\n",
        "scratch_lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_scratch = scratch_lr.predict(X_val)\n",
        "\n",
        "mse_scratch = mean_squared_error(y_val, y_pred_scratch)\n",
        "print(\"Mean Squared Error (Scratch):\", mse_scratch)\n",
        "print(\"Mean Squared Error Comparison:\")\n",
        "print(\"Scratch Implementation:\", mse_scratch)\n",
        "print(\"Scikit-learn Implementation:\", mse_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMtpzqLYSLzJ",
        "outputId": "35271e31-f112-4a16-e43b-2d36d91cb425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Scratch): 0.39560888091490665\n",
            "Mean Squared Error Comparison:\n",
            "Scratch Implementation: 0.39560888091490665\n",
            "Scikit-learn Implementation: 0.39569344443628673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 7"
      ],
      "metadata": {
        "id": "Le5qv6GnTXuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(train_loss, val_loss):\n",
        "    \"\"\"\n",
        "    Plot the learning curve for training and validation loss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_loss : ndarray\n",
        "        Training loss values recorded during training.\n",
        "    val_loss : ndarray\n",
        "        Validation loss values recorded during training.\n",
        "    \"\"\"\n",
        "\n",
        "    iterations = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(iterations, train_loss, label='Training Loss', color='blue')\n",
        "    plt.plot(iterations, val_loss, label='Validation Loss', color='orange')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BDt9jRTITVIV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}